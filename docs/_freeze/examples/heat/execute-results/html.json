{
  "hash": "99937e7217e5d6ced2626d0d1399beda",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Heat Equation\"\nformat: \n    html: \n        toc: true\n        \nbibliography: \"../references.bib\"\njupyter: python3\n---\n\n\n\n\nHere, we characterise the posterior distribution of the diffusion coefficient of a two-dimensional heat equation. We will consider a similar setup to that described in @Cui2022.\n\n# Problem Setup\n\nWe consider the domain $\\Omega := (0, 3) \\times (0, 1)$, with boundary denoted by $\\partial \\Omega$. The change in temperature, $u(\\boldsymbol{x}, t)$, at each point in the domain over time can be modelled by the heat equation,\n\n$$\n    \\frac{\\partial u(\\boldsymbol{x}, t)}{\\partial t} = \\nabla \\cdot (\\kappa(\\boldsymbol{x}) \\nabla u(\\boldsymbol{x}, t)) + f(\\boldsymbol{x}, t), \\quad \\boldsymbol{x} \\in \\Omega, t \\in (0, T],\n$$\n\nwhere $\\kappa(\\boldsymbol{x})$ denotes the (spatially varying) diffusion coefficient, and $f(\\boldsymbol{x}, t)$ denotes the forcing term, which models heat sources or sinks. We set the end time to $T = 10$, and impose the initial and boundary conditions\n\n$$\n\\begin{align}\n    u(\\boldsymbol{x}, 0) &= 0, \\qquad \\boldsymbol{x} \\in \\Omega, \\\\\n    \\frac{\\partial \\kappa(\\boldsymbol{x}) u(\\boldsymbol{x}, t)}{\\partial \\boldsymbol{n}} &= 0, \\qquad \\boldsymbol{x} \\in \\partial\\Omega.\n\\end{align}\n$$\n\nIn the above, $\\boldsymbol{n}$ denotes the outward-facing normal vector on the boundary of the domain.\n\nWe assume that the forcing term is given by\n\n$$\n    f(\\boldsymbol{x}, t) = c \\left(\\exp\\left(−\\frac{1}{2r^{2}}||\\boldsymbol{x} − \\boldsymbol{a}||^{2}\\right) − \\exp\\left(-\\frac{1}{2r^{2}}||\\boldsymbol{x} − \\boldsymbol{b}||^{2}\\right)\\right),\n$$\n\nwhere $\\boldsymbol{a} = \\begin{bmatrix} 1/2, 1/2 \\end{bmatrix}^{\\top}$, $\\boldsymbol{b} = [5/2, 1/2]^{\\top}$, and $c = 5 \\pi \\times 10^{-2}$.\n\n## Prior Density\n\nWe endow the logarithm of the unknown diffusion coefficient with a process convolution prior; that is,\n\n$$\n    \\log(\\kappa(\\boldsymbol{x})) = \\log(\\bar{\\kappa}(\\boldsymbol{x})) + \\sum_{i=1}^{d} \\xi^{(i)} \\exp\\left(-\\frac{1}{2r^{2}}\\left\\lVert\\boldsymbol{x} - \\boldsymbol{x}^{(i)}\\right\\rVert^{2}\\right),\n$$\n\nwhere $d=27$, $\\log(\\bar{\\kappa}(\\boldsymbol{x}))=-5$, $r=1/16$, the coefficients $\\{\\xi^{(i)}\\}_{i=1}^{d}$ are independent and follow the unit Gaussian distribution, and the centres of the kernel functions, $\\{\\boldsymbol{x}^{(i)}\\}_{i=1}^{d}$, form a grid over the domain (see @fig-ktrue).\n\n## Data\n\nTo estimate the diffusivity coefficient, we assume that we have access to measurements of the temperature at 13 locations in the model domain (see @fig-utrue), recorded at one-second intervals. This gives a total of 130 measurements. All measurements are corrupted by i.i.d. Gaussian noise with zero mean and a standard deviation of $\\sigma=1.65 \\times 10^{-2}$.\n\n# Implementation in $\\texttt{deep\\_tensor}$\n\nWe will now use $\\texttt{deep\\_tensor}$ to construct a DIRT approximation to the posterior. To accelerate this process, we will use a reduced order model in place of the full model. Then, we will illustrate some debiasing techniques which use the DIRT approximation to the posterior, in combination with the full model, to accelerate the process of drawing exact posterior samples.\n\n::: {#c970d131 .cell execution_count=1}\n``` {.python .cell-code}\nfrom matplotlib import pyplot as plt\nimport torch\n\nimport deep_tensor as dt\n```\n:::\n\n\n\n\nWe begin by defining the prior, (full) model and reduced order model.\n\nThe full model is implemented in [FEniCS](https://fenicsproject.org/download/archive/), on a $96 \\times 32$ grid, using piecwise linear basis functions. Timestepping is done using the backward Euler method. The reduced order model is constructed using the proper orthogonal decomposition [see, *e.g.*, @Benner2015].\n\n::: {#65e73428 .cell execution_count=3}\n``` {.python .cell-code}\nfrom models.heat import setup_heat_problem\n\n# Construct the prior, full model and reduced order model\nprior, model, rom = setup_heat_problem()\n```\n:::\n\n\nNext, we will generate the true log-diffusion coefficient using a sample from the prior. The true log-diffusion coefficient is plotted in @fig-ktrue.\n\n::: {#8baea6f2 .cell execution_count=4}\n``` {.python .cell-code}\nxi_true = torch.randn((prior.dim,))\nlogk_true = prior.transform(xi_true)\n```\n:::\n\n\n::: {#cell-fig-ktrue .cell execution_count=5}\n``` {.python .cell-code code-fold=\"true\"}\nfig, ax = plt.subplots(figsize=(6.0, 2.0))\ncbar_label = r\"$\\log(\\kappa(\\bm{x}))$\"\nplot_dl_function(fig, ax, model.vec2func(logk_true), cbar_label)\nax.scatter(*prior.ss.T, s=16, c=\"k\", marker=\"x\")\nax.set_xlabel(r\"$x_{0}$\")\nax.set_ylabel(r\"$x_{1}$\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![The true log-diffusion coefficient, $\\log(\\kappa(\\boldsymbol{x}))$, and the centres of the kernel functions of the process convolution prior (black crosses).](heat_files/figure-html/fig-ktrue-output-1.png){#fig-ktrue width=523 height=192 fig-align='center'}\n:::\n:::\n\n\nNext, we will solve the (full) model to obtain the modelled temperatures corresponding to the true diffusion coefficient, and use these to generate some synthetic data. @fig-utrue shows the true temperature field at time $T=10$, as well as the observation locations.\n\n::: {#f5f017da .cell execution_count=6}\n``` {.python .cell-code}\n# Generate true temperature field\nu_true = model.solve(logk_true)\n\n# Specify magnitude of observation noise\nstd_error = 1.65e-2\nvar_error = std_error ** 2\n\n# Extract true temperatures at the observation locations and add \n# observation noise\nd_obs = model.observe(u_true)\nnoise = std_error * torch.randn_like(d_obs)\nd_obs += noise\n```\n:::\n\n\n::: {#cell-fig-utrue .cell execution_count=7}\n``` {.python .cell-code code-fold=\"true\"}\nfig, ax = plt.subplots(figsize=(6.0, 2.0))\ncbar_label = r\"$u(\\bm{x}, 10)$\"\nplot_dl_function(fig, ax, model.vec2func(u_true[:, -1]), cbar_label, vmin=-0.15, vmax=0.1)\nax.scatter(*model.xs_obs.T, s=16, c=\"k\", marker=\".\")\nax.set_xlabel(r\"$x_{0}$\")\nax.set_ylabel(r\"$x_{1}$\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![The true final temperature distribution, $u(\\boldsymbol{x}, 10)$, and the observation locations (black dots).](heat_files/figure-html/fig-utrue-output-1.png){#fig-utrue width=535 height=192 fig-align='center'}\n:::\n:::\n\n\n## Building the DIRT Object\n\nNow we will build a DIRT object to approximate the posterior density of the log-diffusion coefficient for the reduced-order model. We begin by defining functions which return the potential associated with the likelihood and prior.\n\n::: {#20897a1b .cell execution_count=8}\n``` {.python .cell-code}\ndef neglogpri(xs: torch.Tensor) -> torch.Tensor:\n    \"\"\"Returns the negative log prior density evaluated a given set of \n    samples.\n    \"\"\"\n    return 0.5 * xs.square().sum(dim=1)\n\ndef _negloglik(model, xs: torch.Tensor) -> torch.Tensor:\n    \"\"\"Returns the negative log-likelihood, for a given model, \n    evaluated at each of a set of samples.\n    \"\"\"\n    neglogliks = torch.zeros(xs.shape[0])\n    for i, x in enumerate(xs):\n        k = prior.transform(x)\n        us = model.solve(k)\n        d = model.observe(us)\n        neglogliks[i] = 0.5 * (d - d_obs).square().sum() / var_error\n    return neglogliks\n\ndef negloglik(xs: torch.Tensor) -> torch.Tensor:\n    \"\"\"Returns the negative log-likelihood for the full model (to be \n    used later).\n    \"\"\"\n    return _negloglik(model, xs)\n\ndef negloglik_rom(xs: torch.Tensor) -> torch.Tensor:\n    \"\"\"Returns the negative log-likelihood for the reduced-order model.\"\"\"\n    return _negloglik(rom, xs)\n```\n:::\n\n\nNext, we specify a preconditioner. Because the prior of the coefficients $\\{\\xi^{(i)}\\}_{i=1}^{d}$ is the standard Gaussian, the mapping between a Gaussian reference and the prior is simply the identity mapping. This is an appropriate choice of preconditioner in the absence of any other information.\n\n::: {#0e787880 .cell execution_count=9}\n``` {.python .cell-code}\nreference = dt.GaussianReference()\npreconditioner = dt.IdentityMapping(prior.dim, reference)\n```\n:::\n\n\nNext, we specify a polynomial basis.\n\n::: {#d9f51550 .cell execution_count=10}\n``` {.python .cell-code}\npoly = dt.Legendre(order=20)\n```\n:::\n\n\nFinally, we can construct the DIRT object.\n\n::: {#500852ef .cell execution_count=11}\n``` {.python .cell-code}\n# Reduce the initial and maximum tensor ranks to reduce the cost of \n# each layer\ntt_options = dt.TTOptions(init_rank=12, max_rank=12)\n\ndirt = dt.DIRT(\n    negloglik_rom, \n    neglogpri,\n    preconditioner,\n    poly, \n    tt_options=tt_options\n)\n```\n:::\n\n\n\n\n## Debiasing\n\nWe could use the DIRT object directly as an approximation to the target posterior. However, it is also possible to use the DIRT object to accelerate exact inference with the full model.\n\nWe will illustrate two possibilities to remove the bias from the inference results obtained using DIRT; using the DIRT density as part of a Markov chain Monte Carlo (MCMC) sampler, or as a proposal density for importance sampling.\n\n### MCMC Sampling\n\nFirst, we will illustrate how to use the DIRT density as part of an MCMC sampler. The simplest sampler, which we demonstrate here, is an independence sampler using the DIRT density as a proposal density. \n\n::: {#a091d18e .cell execution_count=13}\n``` {.python .cell-code}\n# Generate a set of samples from the DIRT density\nrs = dirt.reference.random(d=dirt.dim, n=5000)\nxs, potentials_dirt = dirt.eval_irt(rs)\n\n# Evaluate the true potential function (for the full model) at each sample\npotentials_exact = neglogpri(xs) + negloglik(xs)\n\n# Run independence sampler\nres = dt.run_independence_sampler(xs, potentials_dirt, potentials_exact)\nprint(f\"Acceptance rate: {res.acceptance_rate:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAcceptance rate: 0.8420\n```\n:::\n:::\n\n\nThe acceptance rate is quite high, which suggests that the DIRT density is a good approximation to the true posterior.\n\n### Importance Sampling\n\nAs an alternative to MCMC, we can also apply importance sampling to reweight samples from the DIRT approximation appropriately.\n\n::: {#a4fbcc75 .cell execution_count=14}\n``` {.python .cell-code}\nres = dt.run_importance_sampling(potentials_dirt, potentials_exact)\nprint(f\"ESS: {res.ess:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nESS: 4600.3042\n```\n:::\n:::\n\n\nAs expected, the effective sample size (ESS) is quite high.\n\n",
    "supporting": [
      "heat_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}