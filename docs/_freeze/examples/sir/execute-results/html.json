{
  "hash": "5ea840ec0dbf2d09108d2db77c3f9239",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"SIR Model\"\nformat: \n    html: \n        toc: true\nbibliography: \"../references.bib\"\njupyter: python3\n---\n\n\n\n\nHere, we characterise the posterior distribution associated with a susceptible-infected-recovered (SIR) model. We will consider a similar setup to that described in @Cui2023.\n\n# Introduction\n\nWe consider the SIR model given by the system of ODEs\n\n$$\n\\frac{\\mathrm{d}S(t)}{\\mathrm{d}t} = -\\beta S I, \\quad \n\\frac{\\mathrm{d}I(t)}{\\mathrm{d}t} = \\beta S I - \\gamma I, \\quad \n\\frac{\\mathrm{d}R(t)}{\\mathrm{d}t} = \\gamma I,\n$$\n\nwhere $S(t)$, $I(t)$ and $R(t)$ denote the number of susceptible, infected and recovered people at time $t$, and $\\beta$ and $\\gamma$ are unknown parameters. For the sake of simplicity, we assume that $S(t)$, $I(t)$ and $R(t)$ can take non-integer values.\n\nWe will assume that the initial conditions for the problem are given by $S(0) = 99$, $I(0) = 1$, $R(0) = 0$, and that we receive four noisy observations of the number of infected people, at times $t \\in \\{1.25, 2.5, 3.75, 5\\}$. We will assume that each of these observations is corrupted by additive, independent Gaussian noise with a mean of $0$ and a standard deviation of $1$. \n\nFinally, we will choose a uniform prior for $\\beta$ and $\\gamma$; that is, $(\\beta, \\gamma) \\sim \\mathcal{U}([0, 2]^{2})$.\n\n# Implementation in $\\mathtt{deep\\_tensor}$\n\nTo solve this inference problem using $\\mathtt{deep\\_tensor}$, we begin by importing the relevant libraries and defining the SIR model.\n\n::: {#d315de6f .cell execution_count=1}\n``` {.python .cell-code}\nimport deep_tensor as dt\nfrom matplotlib import pyplot as plt\nimport torch\n\nfrom models import SIRModel\nmodel = SIRModel()\n```\n:::\n\n\n\n\nNext, we generate some synthetic observations. We will assume that the true values of the parameters are $(\\beta, \\gamma) = (0.1, 1.0)$.\n\n::: {#bff40ef6 .cell execution_count=3}\n``` {.python .cell-code}\nxs_true = torch.tensor([[0.1, 1.0]])\nys_true = model.solve_fwd(xs_true)\nnoise = torch.randn_like(ys_true)\nys_obs = ys_true + noise\n```\n:::\n\n\n## DIRT Construction\n\nThere are several objects we must create prior to building a DIRT approximation to the posterior. Here, we describe the key ones. For a full list, see the [API reference](../reference/index.qmd).\n\n### Likelihood and Prior\n\nWe first define functions that return the potential function (*i.e.*, the negative logarithm) of the likelihood and the prior density.\n\n::: {.callout-note}\nThe `negloglik` and `neglogpri` functions must be able to handle multiple sets of parameters. Each function should accept as input a two-dimensional `torch.Tensor`, where each row contains a sample, and return a one-dimensional `torch.Tensor` object containing the negative log-likelihood, or negative log-prior density, evaluated at each sample.\n:::\n\n::: {#9e68e374 .cell execution_count=4}\n``` {.python .cell-code}\ndef negloglik(xs: torch.Tensor) -> torch.Tensor:\n    ys = model.solve_fwd(xs)\n    return 0.5 * (ys - ys_obs).square().sum(dim=1)\n\ndef neglogpri(xs: torch.Tensor) -> torch.Tensor:\n    neglogpris = torch.full((xs.shape[0],), -torch.tensor(0.25).log())\n    neglogpris[xs[:, 0] < 0.0] = torch.inf \n    neglogpris[xs[:, 1] > 2.0] = torch.inf\n    return neglogpris\n```\n:::\n\n\n### Reference Density and Preconditioner\n\nNext, we specify a product-form reference density. A good choice in most cases is the standard Gaussian density.\n\nWe must also specify a *preconditioner*. Recall that the DIRT object provides a coupling between a product-form reference density and an approximation to the target density. A preconditioner can be considered an initial guess as to what this coupling is.\n\nChoosing an suitable preconditioner can reduce the computational expense required to construct the DIRT object significantly. In the context of a Bayesian inverse problem, a suitable choice is a mapping from the reference density to the prior.\n\n::: {#035a6ee9 .cell execution_count=5}\n``` {.python .cell-code}\nbounds = torch.tensor([[0.0, 2.0], [0.0, 2.0]])\nreference = dt.GaussianReference()\npreconditioner = dt.UniformMapping(bounds, reference)\n```\n:::\n\n\n### Approximation Bases\n\nNext, we specify the polynomial basis which will be used when approximating the marginal PDFs and CDFs required to define the (inverse) Rosenblatt transport. We can specify a list of bases in each dimension, or a single basis (which will be used in all dimensions).\n\nHere, we use a piecewise linear basis with 20 equisized elements in each dimension.\n\n::: {#25af6ebe .cell execution_count=6}\n``` {.python .cell-code}\nbases = dt.Lagrange1(num_elems=20)\n```\n:::\n\n\n### DIRT Object\n\nNow we can construct the DIRT object.\n\n::: {#ee07c4b1 .cell execution_count=7}\n``` {.python .cell-code}\ndirt = dt.DIRT(\n    negloglik, \n    neglogpri, \n    preconditioner, \n    bases,\n    tt_options=dt.TTOptions(verbose=False),\n    dirt_options=dt.DIRTOptions(verbose=False)\n)\n```\n:::\n\n\n@fig-post shows a plot of the DIRT approximation to the posterior, as well as the true posterior for comparison. The posterior is very concentrated in comparison to the prior (particularly for parameter $\\beta$).\n\n::: {#cell-fig-post .cell execution_count=8}\n``` {.python .cell-code code-fold=\"true\"}\nn_grid = 300\nb_grid = torch.linspace(0.05, 0.14, n_grid)\ng_grid = torch.linspace(0.80, 1.40, n_grid)\ngrid = torch.tensor([[b, g] for g in g_grid for b in b_grid])\n\nfig, axes = plt.subplots(1, 2, figsize=(7, 3.5), sharex=True, sharey=True)\n\npdf_true = torch.exp(-(negloglik(grid) + neglogpri(grid)))\npdf_true = pdf_true.reshape(n_grid, n_grid)\n\npdf_dirt = dirt.eval_pdf(grid)\npdf_dirt = pdf_dirt.reshape(n_grid, n_grid)\n\naxes[0].pcolormesh(b_grid, g_grid, pdf_true)\naxes[1].pcolormesh(b_grid, g_grid, pdf_dirt)\n\naxes[0].set_ylabel(r\"$\\gamma$\")\n\nfor ax in axes:\n    ax.set_xlabel(r\"$\\beta$\")\n    ax.set_box_aspect(1)\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![A comparison between the true posterior density (left) and the DIRT approximation (right).](sir_files/figure-html/fig-post-output-1.png){#fig-post width=662 height=336 fig-align='center'}\n:::\n:::\n\n\nWe can sample from the approximation to the posterior by drawing a set of samples from the reference density and calling the `eval_irt` method of the DIRT object. Note that the `eval_irt` method also returns the potential function of the DIRT approximation to the target density evaluated at each sample.\n\n::: {#e06e4f0c .cell execution_count=9}\n``` {.python .cell-code}\nrs = dirt.reference.random(d=dirt.dim, n=20)\nsamples, potentials = dirt.eval_irt(rs)\n```\n:::\n\n\n@fig-samples shows a plot of the samples.\n\n::: {#cell-fig-samples .cell execution_count=10}\n``` {.python .cell-code code-fold=\"true\"}\nfig, ax = plt.subplots(figsize=(7, 3.5), sharex=True, sharey=True)\n\nax.pcolormesh(b_grid, g_grid, pdf_dirt)\nax.scatter(*samples.T, c=\"white\", s=4)\n\nax.set_xlabel(r\"$\\beta$\")\nax.set_ylabel(r\"$\\gamma$\")\nax.set_box_aspect(1)\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Samples from the DIRT approximation to the posterior.](sir_files/figure-html/fig-samples-output-1.png){#fig-samples width=350 height=336 fig-align='center'}\n:::\n:::\n\n\n## Debiasing\n\nWhile the DIRT approximation to the posterior density is quite accurate, we may still wish to correct for bias in the approximation results. We can do this by using the DIRT density as part of a Markov chain Monte Carlo (MCMC) sampler, or as a proposal density for importance sampling.\n\n### MCMC Sampling\n\nFirst, we will illustrate how to use the DIRT density as part of an MCMC sampler. The simplest sampler, which we demonstrate here, is an independence sampler using the DIRT density as a proposal density. \n\n::: {#cf490b77 .cell execution_count=11}\n``` {.python .cell-code}\ndef potential(xs: torch.Tensor) -> torch.Tensor:\n    return negloglik(xs) + neglogpri(xs)\n\nrs = dirt.reference.random(d=dirt.dim, n=1000)\nsamples, potentials_irt = dirt.eval_irt(rs)\npotentials_true = potential(samples)\n\nres = dt.run_independence_sampler(samples, potentials_irt, potentials_true)\nprint(f\"Acceptance rate: {res.acceptance_rate:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAcceptance rate: 0.9100\n```\n:::\n:::\n\n\n@fig-chain shows the first 20 points of the simulated Markov chain.\n\n::: {#cell-fig-chain .cell execution_count=12}\n``` {.python .cell-code code-fold=\"true\"}\nfig, ax = plt.subplots(figsize=(7, 3.5), sharex=True, sharey=True)\n\nax.pcolormesh(b_grid, g_grid, pdf_true)\nax.scatter(*res.xs[:20].T, c=\"white\", s=4)\nax.plot(*res.xs[:20].T, c=\"white\", lw=0.4)\n\nax.set_xlabel(r\"$\\beta$\")\nax.set_ylabel(r\"$\\gamma$\")\nax.set_box_aspect(1)\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![The first 20 points of the simulated Markov chain, plotted on top of the true posterior density.](sir_files/figure-html/fig-chain-output-1.png){#fig-chain width=350 height=336 fig-align='center'}\n:::\n:::\n\n\n### Importance Sampling\n\nAs an alternative to MCMC, we can also apply importance sampling to reweight samples from the DIRT approximation appropriately.\n\n::: {#f72ad7b9 .cell execution_count=13}\n``` {.python .cell-code}\nres = dt.run_importance_sampling(potentials_irt, potentials_true)\nprint(f\"ESS: {res.ess:.2f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nESS: 976.20\n```\n:::\n:::\n\n\nAs expected, the effective sample size (ESS) is almost equal to the number of samples.\n\n",
    "supporting": [
      "sir_files"
    ],
    "filters": [],
    "includes": {}
  }
}