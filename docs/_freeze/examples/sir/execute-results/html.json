{
  "hash": "82f11a55ec7838db191ccae553d9e095",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"SIR Model\"\nformat: \n    html: \n        toc: true\nbibliography: \"../references.bib\"\njupyter: python3\n---\n\n\n\n\nHere, we characterise the posterior distribution associated with a susceptible-infectious-recovered (SIR) model. We will consider a similar setup to that described in @Cui2023.\n\n# Problem Setup\n\nWe consider the SIR model given by the system of ODEs\n\n$$\n\\frac{\\mathrm{d}S(t)}{\\mathrm{d}t} = -\\beta S I, \\quad \n\\frac{\\mathrm{d}I(t)}{\\mathrm{d}t} = \\beta S I - \\gamma I, \\quad \n\\frac{\\mathrm{d}R(t)}{\\mathrm{d}t} = \\gamma I,\n$$\n\nwhere $S(t)$, $I(t)$ and $R(t)$ denote the number of susceptible, infectious and recovered people at time $t$, and $\\beta$ and $\\gamma$ are unknown parameters. For the sake of simplicity, we assume that $S(t)$, $I(t)$ and $R(t)$ can take non-integer values.\n\nWe will assume that the initial conditions for the problem are given by $S(0) = 99$, $I(0) = 1$, $R(0) = 0$, and that we receive four noisy observations of the number of infectious people, at times $t \\in \\{1.25, 2.5, 3.75, 5\\}$. We will assume that each of these observations is corrupted by additive, independent Gaussian noise with a mean of $0$ and a standard deviation of $1$. \n\nFinally, we will choose a uniform prior for $\\beta$ and $\\gamma$; that is, $(\\beta, \\gamma) \\sim \\mathcal{U}([0, 2]^{2})$.\n\n# Implementation in $\\mathtt{deep\\_tensor}$\n\nTo solve this inference problem using $\\mathtt{deep\\_tensor}$, we begin by importing the relevant libraries and defining the SIR model.\n\n::: {#5313ec97 .cell execution_count=1}\n``` {.python .cell-code}\nfrom matplotlib import pyplot as plt\nimport torch\n\nimport deep_tensor as dt\n\nfrom models import SIRModel\nmodel = SIRModel()\n```\n:::\n\n\n\n\nNext, we generate some synthetic observations. We will assume that the true values of the parameters are $(\\beta, \\gamma) = (0.1, 1.0)$.\n\n::: {#59789c61 .cell execution_count=3}\n``` {.python .cell-code}\nxs_true = torch.tensor([[0.1, 1.0]])\nys_true = model.solve_fwd(xs_true)\nnoise = torch.randn_like(ys_true)\nys_obs = ys_true + noise\n```\n:::\n\n\n## DIRT Construction\n\nThere are several objects we must create prior to building a DIRT approximation to the posterior. Here, we describe the key ones. For a full list, see the [API reference](../reference/index.qmd).\n\n### Likelihood and Prior\n\nWe first define functions that return the potential function (*i.e.*, the negative logarithm) of the likelihood and the prior density.\n\n::: {.callout-note}\nThe `negloglik` and `neglogpri` functions must be able to handle multiple sets of parameters. Each function should accept as input a two-dimensional `torch.Tensor`, where each row contains a sample, and return a one-dimensional `torch.Tensor` object containing the negative log-likelihood, or negative log-prior density, evaluated at each sample.\n:::\n\n::: {#1a73747c .cell execution_count=4}\n``` {.python .cell-code}\ndef negloglik(xs: torch.Tensor) -> torch.Tensor:\n    ys = model.solve_fwd(xs)\n    return 0.5 * (ys - ys_obs).square().sum(dim=1)\n\ndef neglogpri(xs: torch.Tensor) -> torch.Tensor:\n    neglogpris = torch.full((xs.shape[0],), -torch.tensor(0.25).log())\n    neglogpris[xs[:, 0] < 0.0] = torch.inf \n    neglogpris[xs[:, 1] > 2.0] = torch.inf\n    return neglogpris\n```\n:::\n\n\n### Reference Density and Preconditioner\n\nNext, we specify a product-form reference density. A suitable choice in most cases is the standard Gaussian density.\n\nWe must also specify a *preconditioner*. Recall that the DIRT object provides a coupling between a product-form reference density and an approximation to the target density. A preconditioner can be considered an initial guess as to what this coupling is.\n\nChoosing an suitable preconditioner can reduce the computational expense required to construct the DIRT object significantly. In the context of a Bayesian inverse problem, a suitable choice is a mapping from the reference density to the prior.\n\n::: {#85e53ef7 .cell execution_count=5}\n``` {.python .cell-code}\nbounds = torch.tensor([[0.0, 2.0], [0.0, 2.0]])\nreference = dt.GaussianReference()\npreconditioner = dt.UniformMapping(bounds, reference)\n```\n:::\n\n\n### Approximation Bases\n\nNext, we specify the polynomial basis which will be used when approximating the marginal PDFs and CDFs required to define the (inverse) Rosenblatt transport. We can specify a list of bases in each dimension, or a single basis (which will be used in all dimensions).\n\nHere, we use a basis comprised of Legendre polynomials with a maximum degree of 30 in each dimension.\n\n::: {#7be19a12 .cell execution_count=6}\n``` {.python .cell-code}\nbases = dt.Legendre(order=30)\n```\n:::\n\n\n### DIRT Object\n\nNow we can construct the DIRT object.\n\n::: {#ee521818 .cell execution_count=7}\n``` {.python .cell-code}\ndirt = dt.DIRT(negloglik, neglogpri, preconditioner, bases)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[DIRT] Iter:  1 | Cum. Fevals: 2.00e+03 | Cum. Time: 2.54e-01 s | Beta: 0.0001 | ESS: 0.9745\n[ALS]  Iter | Func Evals | Max Rank | Max Local Error | Mean Local Error | Max Debug Error | Mean Debug Error\n[ALS]     1 |        930 |        8 |     1.00000e+00 |      1.00000e+00 |     5.21830e-03 |      1.08928e-03\n[ALS]  ALS complete. Final TT ranks: 8-1.\n[DIRT] Iter:  2 | Cum. Fevals: 5.86e+03 | Cum. Time: 8.80e-01 s | Beta: 0.0037 | ESS: 0.5125 | DHell: 0.0013\n[ALS]  Iter | Func Evals | Max Rank | Max Local Error | Mean Local Error | Max Debug Error | Mean Debug Error\n[ALS]     1 |        620 |       10 |     1.39594e+00 |      1.39594e+00 |     2.18277e-01 |      4.29492e-02\n[ALS]  ALS complete. Final TT ranks: 10-1.\n[DIRT] Iter:  3 | Cum. Fevals: 9.10e+03 | Cum. Time: 1.51e+00 s | Beta: 0.0204 | ESS: 0.5077 | DHell: 0.0274\n[ALS]  Iter | Func Evals | Max Rank | Max Local Error | Mean Local Error | Max Debug Error | Mean Debug Error\n[ALS]     1 |        744 |       12 |     2.50977e-01 |      2.50977e-01 |     4.73993e-02 |      2.83657e-02\n[ALS]  ALS complete. Final TT ranks: 12-1.\n[DIRT] Iter:  4 | Cum. Fevals: 1.26e+04 | Cum. Time: 2.09e+00 s | Beta: 0.0627 | ESS: 0.5412 | DHell: 0.0172\n[ALS]  Iter | Func Evals | Max Rank | Max Local Error | Mean Local Error | Max Debug Error | Mean Debug Error\n[ALS]     1 |        868 |       14 |     3.15719e-01 |      3.15719e-01 |     2.76007e-02 |      1.76338e-02\n[ALS]  ALS complete. Final TT ranks: 14-1.\n[DIRT] Iter:  5 | Cum. Fevals: 1.63e+04 | Cum. Time: 2.53e+00 s | Beta: 0.1303 | ESS: 0.5262 | DHell: 0.0149\n[ALS]  Iter | Func Evals | Max Rank | Max Local Error | Mean Local Error | Max Debug Error | Mean Debug Error\n[ALS]     1 |        992 |       16 |     1.73862e-01 |      1.73862e-01 |     3.09862e-02 |      1.33210e-02\n[ALS]  ALS complete. Final TT ranks: 16-1.\n[DIRT] Iter:  6 | Cum. Fevals: 2.03e+04 | Cum. Time: 3.03e+00 s | Beta: 0.3292 | ESS: 0.5158 | DHell: 0.0157\n[ALS]  Iter | Func Evals | Max Rank | Max Local Error | Mean Local Error | Max Debug Error | Mean Debug Error\n[ALS]     1 |       1116 |       18 |     4.13233e-01 |      4.13233e-01 |     8.46778e-03 |      7.16653e-03\n[ALS]  ALS complete. Final TT ranks: 18-1.\n[DIRT] Iter:  7 | Cum. Fevals: 2.45e+04 | Cum. Time: 3.49e+00 s | Beta: 1.0000 | ESS: 0.5663 | DHell: 0.0114\n[ALS]  Iter | Func Evals | Max Rank | Max Local Error | Mean Local Error | Max Debug Error | Mean Debug Error\n[ALS]     1 |       1023 |       13 |     3.34057e-01 |      3.34057e-01 |     1.14751e-02 |      5.64585e-03\n[ALS]  ALS complete. Final TT ranks: 13-1.\n[DIRT] DIRT construction complete.\n[DIRT]  • Layers: 7.\n[DIRT]  • Total function evaluations: 28586.\n[DIRT]  • Total time: 4.18 s.\n[DIRT]  • DHell: 0.0093\n```\n:::\n:::\n\n\nObserve that a set of diagnostic information is printed at each stage of DIRT construction.\n\n## Sampling, Marginalisation and Conditioning\n\nWe now illustrate how to use the DIRT approximation to carry out a range of tasks.\n\n### Sampling\n\nFirst, it is possible to evaluate the DIRT approximation to the target density pointwise. The below code evaluates the potential function associated with the DIRT approximation to the target density, on a grid of $\\beta$ and $\\gamma$ values.\n\n::: {#f1f8c91d .cell execution_count=8}\n``` {.python .cell-code}\n# Define grid to evaluate potential function on\nn_grid = 200\nbeta_grid = torch.linspace(0.05, 0.14, n_grid)\ngamma_grid = torch.linspace(0.80, 1.40, n_grid)\ngrid = torch.tensor([[b, g] for g in gamma_grid for b in beta_grid])\n\n# Evaluate potential function\npotentials_grid = dirt.eval_potential(grid)\n```\n:::\n\n\n@fig-post shows a plot of the DIRT density evaluated on the above grid and compares it to the true posterior. The posterior is very concentrated in comparison to the prior (particularly for parameter $\\beta$).\n\n::: {#cell-fig-post .cell execution_count=9}\n``` {.python .cell-code code-fold=\"true\"}\nfig, axes = plt.subplots(1, 2, figsize=(7, 3.5), sharex=True, sharey=True)\n\n# Compute true density\npdf_true = torch.exp(-(negloglik(grid) + neglogpri(grid)))\npdf_true = pdf_true.reshape(n_grid, n_grid)\n\n# Normalise true density\ndb = beta_grid[1] - beta_grid[0]\ndg = gamma_grid[1] - gamma_grid[0]\npdf_true /= (pdf_true.sum() * db * dg)\n\n# Compute DIRT approximation\npdf_dirt = torch.exp(-potentials_grid)\npdf_dirt = pdf_dirt.reshape(n_grid, n_grid)\n\naxes[0].pcolormesh(beta_grid, gamma_grid, pdf_true)\naxes[1].pcolormesh(beta_grid, gamma_grid, pdf_dirt)\naxes[0].set_ylabel(r\"$\\gamma$\")\nfor ax in axes:\n    ax.set_xlabel(r\"$\\beta$\")\n    ax.set_box_aspect(1)\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![A comparison between the true posterior density (left) and the DIRT approximation (right).](sir_files/figure-html/fig-post-output-1.png){#fig-post width=662 height=336 fig-align='center'}\n:::\n:::\n\n\nWe can sample from the DIRT density by drawing a set of samples from the reference density and calling the `eval_irt` method of the DIRT object. Note that the `eval_irt` method also returns the potential function of the DIRT density evaluated at each sample.\n\n::: {#5aa42602 .cell execution_count=10}\n``` {.python .cell-code}\nrs = dirt.reference.random(d=dirt.dim, n=20)\nsamples, potentials = dirt.eval_irt(rs)\n```\n:::\n\n\n@fig-samples shows a plot of the samples.\n\n::: {#cell-fig-samples .cell execution_count=11}\n``` {.python .cell-code code-fold=\"true\"}\nfig, ax = plt.subplots(figsize=(7, 3.5), sharex=True, sharey=True)\n\nax.pcolormesh(beta_grid, gamma_grid, pdf_dirt)\nax.scatter(*samples.T, c=\"white\", s=4)\nax.set_xlabel(r\"$\\beta$\")\nax.set_ylabel(r\"$\\gamma$\")\nax.set_box_aspect(1)\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Samples from the DIRT approximation to the posterior.](sir_files/figure-html/fig-samples-output-1.png){#fig-samples width=350 height=336 fig-align='center'}\n:::\n:::\n\n\n### Marginalisation\n\nWe can also sample from and evaluate specific marginal densities. In the case of a multi-layered DIRT, we can evaluate the (normalised) DIRT approximation to the marginal density of the first $k$ variables, where $1 \\leq k \\leq d$ (where $d$ denotes the dimension of the target random variable).\n\nThe below code generates a set of samples from the marginal density of parameter $\\beta$, and evaluates the marginal density on a grid of $\\beta$ values.\n\n::: {#cb51af0a .cell execution_count=12}\n``` {.python .cell-code}\n# Generate marginal samples of parameter beta\nrs_beta = dirt.reference.random(d=1, n=1000)\nsamples_beta, potentials_beta = dirt.eval_irt(rs_beta, subset=\"first\")\n\n# Evaluate marginal potential on the grid of beta values defined previously\npotentials_grid = dirt.eval_potential(beta_grid[:, None], subset=\"first\")\n```\n:::\n\n\n@fig-marginal plots the samples of $\\beta$, and provides a comparison between the DIRT approximation to the density and the true density.\n\n::: {#cell-fig-marginal .cell execution_count=13}\n``` {.python .cell-code code-fold=\"true\"}\npdf_true_marg = pdf_true.sum(dim=0) * dg\npdf_dirt_marg = torch.exp(-potentials_grid)\n\nfig, ax = plt.subplots(figsize=(6.5, 3.5))\n\nax.plot(beta_grid, pdf_true_marg, c=\"k\", label=r\"True density\", zorder=2)\nax.plot(beta_grid, pdf_dirt_marg, c=\"tab:green\", ls=\"--\", label=r\"DIRT density\", zorder=3)\nax.hist(samples_beta, color=\"tab:green\", density=True, alpha=0.5, zorder=1, label=\"Samples\")\nax.set_xlabel(r\"$\\beta$\")\nax.set_ylabel(r\"$f(\\beta)$\")\nax.set_box_aspect(1)\nax.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\nadd_arrows(ax)\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![A comparison between the true marginal density of $\\beta$ and the DIRT approximation.](sir_files/figure-html/fig-marginal-output-1.png){#fig-marginal width=521 height=336 fig-align='center'}\n:::\n:::\n\n\n### Conditioning\n\nFinally, we can sample from and evaluate specific conditional densities. In the case of a multi-layered DIRT, we can evaluate the (normalised) DIRT approximation to the conditional density of the final $(d-k)$ variables conditioned on the first $k$ variables, where $1 \\leq k < d$ (where $d$ denotes the dimension of the target random variable).\n\nThe below code generates a set of samples from the density of $\\gamma$ conditioned on a value of $\\beta=0.1$, and evaluates the conditional density on a grid of $\\gamma$ values.\n\n::: {#21cdf0a7 .cell execution_count=14}\n``` {.python .cell-code}\n# Define beta value to condition on\nbeta_cond = torch.tensor([[0.10]])\n\n# Generate conditional samples of gamma\nrs_cond = dirt.reference.random(d=1, n=1000)\nsamples_gamma, potentials_gamma = dirt.eval_cirt(beta_cond, rs_cond, subset=\"first\")\n\n# Evaluate conditional potential on a grid of gamma values\ngamma_grid = torch.linspace(0.9, 1.1, 200)[:, None]\npotentials_grid = dirt.eval_potential_cond(beta_cond, gamma_grid, subset=\"first\")\n```\n:::\n\n\n@fig-conditional plots the conditional samples of $\\gamma$, and provides a comparison between the DIRT approximation to the conditional density and the true density.\n\n::: {#cell-fig-conditional .cell execution_count=15}\n``` {.python .cell-code code-fold=\"true\"}\nbeta_cond = beta_cond.repeat(gamma_grid.shape[0], 1)\ngrid_cond = torch.hstack((beta_cond, gamma_grid))\ndg = gamma_grid[1] - gamma_grid[0]\n\n# Evaluate true conditional density\npdf_true_cond = torch.exp(-(negloglik(grid_cond) + neglogpri(grid_cond))).flatten()\npdf_dirt_cond = torch.exp(-potentials_grid)\n\n# Normalise true conditional density\npdf_true_cond /= (pdf_true_cond.sum() * dg)\n\nfig, ax = plt.subplots(figsize=(6.5, 3.5))\n\nax.plot(gamma_grid, pdf_true_cond, c=\"k\", label=r\"True density\", zorder=3)\nax.plot(gamma_grid, pdf_dirt_cond, c=\"tab:purple\", ls=\"--\", label=r\"DIRT density\", zorder=3)\nax.hist(samples_gamma, color=\"tab:purple\", density=True, alpha=0.5, zorder=1, label=\"Samples\")\nax.set_xlabel(r\"$\\gamma$\")\nax.set_ylabel(r\"$f(\\gamma|\\beta=0.1)$\")\nax.set_box_aspect(1)\nax.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\nadd_arrows(ax)\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![A comparison between the true conditional density of $\\gamma | \\beta = 0.1$ and the DIRT approximation.](sir_files/figure-html/fig-conditional-output-1.png){#fig-conditional width=533 height=336 fig-align='center'}\n:::\n:::\n\n\n",
    "supporting": [
      "sir_files"
    ],
    "filters": [],
    "includes": {}
  }
}