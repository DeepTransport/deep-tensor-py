---
title: "Heat Equation"
format: 
    html: 
        toc: true
        
bibliography: "../references.bib"
jupyter: python3
---

Here, we characterise the posterior distribution of the diffusion coefficient of a two-dimensional heat equation. We will consider a similar setup to that described in @Cui2022.

# Problem Setup

We consider the domain $\Omega := (0, 3) \times (0, 1)$, with boundary denoted by $\partial \Omega$. The change in temperature, $u(\boldsymbol{x}, t)$, at each point in the domain over time can be modelled by the heat equation,

$$
    \frac{\partial u(\boldsymbol{x}, t)}{\partial t} = \nabla \cdot (\kappa(\boldsymbol{x}) \nabla u(\boldsymbol{x}, t)) + f(\boldsymbol{x}, t), \quad \boldsymbol{x} \in \Omega, t \in (0, T],
$$

where $\kappa(\boldsymbol{x})$ denotes the (spatially varying) diffusion coefficient, and $f(\boldsymbol{x}, t)$ denotes the forcing term, which models heat sources or sinks. We set the end time to $T = 10$, and impose the initial and boundary conditions

$$
\begin{align}
    u(\boldsymbol{x}, 0) &= 0, \qquad \boldsymbol{x} \in \Omega, \\
    \frac{\partial \kappa(\boldsymbol{x}) u(\boldsymbol{x}, t)}{\partial \boldsymbol{n}} &= 0, \qquad \boldsymbol{x} \in \partial\Omega.
\end{align}
$$

In the above, $\boldsymbol{n}$ denotes the outward-facing normal vector on the boundary of the domain.

We assume that the forcing term is given by

$$
    f(\boldsymbol{x}, t) = c \left(\exp\left(−\frac{1}{2r^{2}}||\boldsymbol{x} − \boldsymbol{a}||^{2}\right) − \exp\left(-\frac{1}{2r^{2}}||\boldsymbol{x} − \boldsymbol{b}||^{2}\right)\right),
$$

where $\boldsymbol{a} = \begin{bmatrix} 1/2, 1/2 \end{bmatrix}^{\top}$, $\boldsymbol{b} = [5/2, 1/2]^{\top}$, and $c = 5 \pi \times 10^{-2}$.

## Prior Density

We endow the logarithm of the unknown diffusion coefficient with a process convolution prior; that is,

$$
    \log(\kappa(\boldsymbol{x})) = \log(\bar{\kappa}(\boldsymbol{x})) + \sum_{i=1}^{d} \xi^{(i)} \exp\left(-\frac{1}{2r^{2}}\left\lVert\boldsymbol{x} - \boldsymbol{x}^{(i)}\right\rVert^{2}\right),
$$

where $d=27$, $\log(\bar{\kappa}(\boldsymbol{x}))=-5$, $r=1/16$, the coefficients $\{\xi^{(i)}\}_{i=1}^{d}$ are independent and follow the unit Gaussian distribution, and the centres of the kernel functions, $\{\boldsymbol{x}^{(i)}\}_{i=1}^{d}$, form a grid over the domain (see @fig-ktrue).

## Data

To estimate the diffusivity coefficient, we assume that we have access to measurements of the temperature at 13 locations in the model domain (see @fig-utrue), recorded at one-second intervals. This gives a total of 130 measurements. All measurements are corrupted by i.i.d. Gaussian noise with zero mean and a standard deviation of $\sigma=1.65 \times 10^{-2}$.

# Implementation in $\texttt{deep\_tensor}$

We will now use $\texttt{deep\_tensor}$ to construct a DIRT approximation to the posterior. To accelerate this process, we will use a reduced order model in place of the full model. Then, we will illustrate some debiasing techniques which use the DIRT approximation to the posterior, in combination with the full model, to accelerate the process of drawing exact posterior samples.

```{python}

from matplotlib import pyplot as plt
import torch

import deep_tensor as dt

```

```{python}
#| echo: false
#| output: false

plt.style.use("plotstyle.mplstyle")
torch.manual_seed(1)
from plotting import plot_dl_function

```

We begin by defining the prior, (full) model and reduced order model.

The full model is implemented in [FEniCS](https://fenicsproject.org/download/archive/), on a $96 \times 32$ grid, using piecwise linear basis functions. Timestepping is done using the backward Euler method. The reduced order model is constructed using the proper orthogonal decomposition [see, *e.g.*, @Benner2015].

```{python}
#| output: false

from models.heat import setup_heat_problem

# Construct the prior, full model and reduced order model
prior, model, rom = setup_heat_problem()

```

Next, we will generate the true log-diffusion coefficient using a sample from the prior. The true log-diffusion coefficient is plotted in @fig-ktrue.

```{python}

xi_true = torch.randn((prior.dim,))
logk_true = prior.transform(xi_true)

```

```{python}
#| code-fold: true
#| fig-align: center
#| fig-cap: The true log-diffusion coefficient, $\log(\kappa(\boldsymbol{x}))$, and the centres of the kernel functions of the process convolution prior (black crosses).
#| label: fig-ktrue

fig, ax = plt.subplots(figsize=(6.0, 2.0))
cbar_label = r"$\log(\kappa(\bm{x}))$"
plot_dl_function(fig, ax, model.vec2func(logk_true), cbar_label)
ax.scatter(*prior.ss.T, s=16, c="k", marker="x")
ax.set_xlabel(r"$x_{0}$")
ax.set_ylabel(r"$x_{1}$")
plt.show()

```

Next, we will solve the (full) model to obtain the modelled temperatures corresponding to the true diffusion coefficient, and use these to generate some synthetic data. @fig-utrue shows the true temperature field at time $T=10$, as well as the observation locations.

```{python}

# Generate true temperature field
u_true = model.solve(logk_true)

# Specify magnitude of observation noise
std_error = 1.65e-2
var_error = std_error ** 2

# Extract true temperatures at the observation locations and add 
# observation noise
d_obs = model.observe(u_true)
noise = std_error * torch.randn_like(d_obs)
d_obs += noise

```

```{python}
#| code-fold: true
#| fig-align: center
#| fig-cap: The true final temperature distribution, $u(\boldsymbol{x}, 10)$, and the observation locations (black dots).
#| label: fig-utrue

fig, ax = plt.subplots(figsize=(6.0, 2.0))
cbar_label = r"$u(\bm{x}, 10)$"
plot_dl_function(fig, ax, model.vec2func(u_true[:, -1]), cbar_label, vmin=-0.15, vmax=0.1)
ax.scatter(*model.xs_obs.T, s=16, c="k", marker=".")
ax.set_xlabel(r"$x_{0}$")
ax.set_ylabel(r"$x_{1}$")
plt.show()

```

## Building the DIRT Object

Now we will build a DIRT object to approximate the posterior density of the log-diffusion coefficient for the reduced-order model. We begin by defining functions which return the potential associated with the likelihood and prior.

```{python}

def neglogpri(xs: torch.Tensor) -> torch.Tensor:
    """Returns the negative log prior density evaluated a given set of 
    samples.
    """
    return 0.5 * xs.square().sum(dim=1)

def _negloglik(model, xs: torch.Tensor) -> torch.Tensor:
    """Returns the negative log-likelihood, for a given model, 
    evaluated at each of a set of samples.
    """
    neglogliks = torch.zeros(xs.shape[0])
    for i, x in enumerate(xs):
        k = prior.transform(x)
        us = model.solve(k)
        d = model.observe(us)
        neglogliks[i] = 0.5 * (d - d_obs).square().sum() / var_error
    return neglogliks

def negloglik(xs: torch.Tensor) -> torch.Tensor:
    """Returns the negative log-likelihood for the full model (to be 
    used later).
    """
    return _negloglik(model, xs)

def negloglik_rom(xs: torch.Tensor) -> torch.Tensor:
    """Returns the negative log-likelihood for the reduced-order model."""
    return _negloglik(rom, xs)

```

Next, we specify a preconditioner. Because the prior of the coefficients $\{\xi^{(i)}\}_{i=1}^{d}$ is the standard Gaussian, the mapping between a Gaussian reference and the prior is simply the identity mapping. This is an appropriate choice of preconditioner in the absence of any other information.

```{python}

reference = dt.GaussianReference()
preconditioner = dt.IdentityMapping(prior.dim, reference)

```

Next, we specify a polynomial basis.

```{python}

poly = dt.Legendre(order=20)

```

Finally, we can construct the DIRT object.

```{python}
#| eval: false

# Reduce the initial and maximum tensor ranks to reduce the cost of each layer
tt_options = dt.TTOptions(init_rank=12, max_rank=12)

dirt = dt.DIRT(
    negloglik_rom, 
    neglogpri,
    preconditioner,
    poly, 
    tt_options=tt_options
)

```

```{python}
#| echo: false 

# dirt.save("dirt-rom-legendre")
dirt = dt.SavedDIRT("dirt-rom-legendre", preconditioner)
```

## Debiasing

We could use the DIRT object directly as an approximation to the target posterior. However, it is also possible to use the DIRT object to accelerate exact inference with the full model.

We will illustrate two possibilities to remove the bias from the inference results obtained using DIRT; using the DIRT density as part of a Markov chain Monte Carlo (MCMC) sampler, or as a proposal density for importance sampling.

### MCMC Sampling

First, we will illustrate how to use the DIRT density as part of an MCMC sampler. The simplest sampler, which we demonstrate here, is an independence sampler using the DIRT density as a proposal density. 

```{python}

# Generate a set of samples from the DIRT density
rs = dirt.reference.random(d=dirt.dim, n=5000)
xs, potentials_dirt = dirt.eval_irt(rs)

# Evaluate the true potential function (for the full model) at each sample
potentials_exact = neglogpri(xs) + negloglik(xs)

# Run independence sampler
res = dt.run_independence_sampler(xs, potentials_dirt, potentials_exact)
print(f"Acceptance rate: {res.acceptance_rate:.4f}")

```

The acceptance rate is quite high, which suggests that the DIRT density is a good approximation to the true posterior.

### Importance Sampling

As an alternative to MCMC, we can also apply importance sampling to reweight samples from the DIRT approximation appropriately.

```{python}

res = dt.run_importance_sampling(potentials_dirt, potentials_exact)
print(f"ESS: {res.ess:.4f}")

```

As expected, the effective sample size (ESS) is quite high.