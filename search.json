[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Installation",
    "section": "",
    "text": "The \\(\\texttt{deep\\_tensor}\\) package contains a PyTorch implementation of the deep inverse Rosenblatt transport (DIRT) algorithm introduced by Cui and Dolgov (2022).\n\nInstallation\nComing soon…\n\n\nGetting Started\nCheck out the examples page and API reference for help getting started with \\(\\texttt{deep\\_tensor}\\).\n\n\n\n\n\nReferences\n\nCui, Tiangang, and Sergey Dolgov. 2022. “Deep Composition of Tensor-Trains Using Squared Inverse Rosenblatt Transports.” Foundations of Computational Mathematics 22 (6): 1863–1922."
  },
  {
    "objectID": "examples/sir.html",
    "href": "examples/sir.html",
    "title": "SIR Model",
    "section": "",
    "text": "Here, we characterise the posterior distribution associated with a susceptible-infectious-recovered (SIR) model. We will consider a similar setup to that described in Cui, Dolgov, and Zahm (2023).",
    "crumbs": [
      "Examples",
      "SIR Model"
    ]
  },
  {
    "objectID": "examples/sir.html#dirt-construction",
    "href": "examples/sir.html#dirt-construction",
    "title": "SIR Model",
    "section": "DIRT Construction",
    "text": "DIRT Construction\nThere are several objects we must create prior to building a DIRT approximation to the posterior. Here, we describe the key ones. For a full list, see the API reference.\n\nLikelihood and Prior\nWe first define functions that return the potential function (i.e., the negative logarithm) of the likelihood and the prior density.\n\n\n\n\n\n\nNote\n\n\n\nThe negloglik and neglogpri functions must be able to handle multiple sets of parameters. Each function should accept as input a two-dimensional torch.Tensor, where each row contains a sample, and return a one-dimensional torch.Tensor object containing the negative log-likelihood, or negative log-prior density, evaluated at each sample.\n\n\n\ndef negloglik(xs: torch.Tensor) -&gt; torch.Tensor:\n    ys = model.solve_fwd(xs)\n    return 0.5 * (ys - ys_obs).square().sum(dim=1)\n\ndef neglogpri(xs: torch.Tensor) -&gt; torch.Tensor:\n    neglogpris = torch.full((xs.shape[0],), -torch.tensor(0.25).log())\n    neglogpris[xs[:, 0] &lt; 0.0] = torch.inf \n    neglogpris[xs[:, 1] &gt; 2.0] = torch.inf\n    return neglogpris\n\n\n\nReference Density and Preconditioner\nNext, we specify a product-form reference density. A good choice in most cases is the standard Gaussian density.\nWe must also specify a preconditioner. Recall that the DIRT object provides a coupling between a product-form reference density and an approximation to the target density. A preconditioner can be considered an initial guess as to what this coupling is.\nChoosing an suitable preconditioner can reduce the computational expense required to construct the DIRT object significantly. In the context of a Bayesian inverse problem, a suitable choice is a mapping from the reference density to the prior.\n\nbounds = torch.tensor([[0.0, 2.0], [0.0, 2.0]])\nreference = dt.GaussianReference()\npreconditioner = dt.UniformMapping(bounds, reference)\n\n\n\nApproximation Bases\nNext, we specify the polynomial basis which will be used when approximating the marginal PDFs and CDFs required to define the (inverse) Rosenblatt transport. We can specify a list of bases in each dimension, or a single basis (which will be used in all dimensions).\nHere, we use a piecewise linear basis with 20 equisized elements in each dimension.\n\nbases = dt.Lagrange1(num_elems=20)\n\n\n\nDIRT Object\nNow we can construct the DIRT object.\n\ndirt = dt.DIRT(\n    negloglik, \n    neglogpri, \n    preconditioner, \n    bases,\n    tt_options=dt.TTOptions(verbose=False),\n    dirt_options=dt.DIRTOptions(verbose=False)\n)\n\nFigure 1 shows a plot of the DIRT approximation to the posterior, as well as the true posterior for comparison. The posterior is very concentrated in comparison to the prior (particularly for parameter \\(\\beta\\)).\n\n\nCode\nn_grid = 300\nb_grid = torch.linspace(0.05, 0.14, n_grid)\ng_grid = torch.linspace(0.80, 1.40, n_grid)\ngrid = torch.tensor([[b, g] for g in g_grid for b in b_grid])\n\nfig, axes = plt.subplots(1, 2, figsize=(7, 3.5), sharex=True, sharey=True)\n\npdf_true = torch.exp(-(negloglik(grid) + neglogpri(grid)))\npdf_true = pdf_true.reshape(n_grid, n_grid)\n\npdf_dirt = dirt.eval_pdf(grid)\npdf_dirt = pdf_dirt.reshape(n_grid, n_grid)\n\naxes[0].pcolormesh(b_grid, g_grid, pdf_true)\naxes[1].pcolormesh(b_grid, g_grid, pdf_dirt)\n\naxes[0].set_ylabel(r\"$\\gamma$\")\n\nfor ax in axes:\n    ax.set_xlabel(r\"$\\beta$\")\n    ax.set_box_aspect(1)\n\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: A comparison between the true posterior density (left) and the DIRT approximation (right).\n\n\n\n\n\nWe can sample from the approximation to the posterior by drawing a set of samples from the reference density and calling the eval_irt method of the DIRT object. Note that the eval_irt method also returns the potential function of the DIRT approximation to the target density evaluated at each sample.\n\nrs = dirt.reference.random(d=dirt.dim, n=20)\nsamples, potentials = dirt.eval_irt(rs)\n\nFigure 2 shows a plot of the samples.\n\n\nCode\nfig, ax = plt.subplots(figsize=(7, 3.5), sharex=True, sharey=True)\n\nax.pcolormesh(b_grid, g_grid, pdf_dirt)\nax.scatter(*samples.T, c=\"white\", s=4)\n\nax.set_xlabel(r\"$\\beta$\")\nax.set_ylabel(r\"$\\gamma$\")\nax.set_box_aspect(1)\n\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2: Samples from the DIRT approximation to the posterior.",
    "crumbs": [
      "Examples",
      "SIR Model"
    ]
  },
  {
    "objectID": "examples/sir.html#debiasing",
    "href": "examples/sir.html#debiasing",
    "title": "SIR Model",
    "section": "Debiasing",
    "text": "Debiasing\nWhile the DIRT approximation to the posterior density is quite accurate, we may still wish to correct for bias in the approximation results. We can do this by using the DIRT density as part of a Markov chain Monte Carlo (MCMC) sampler, or as a proposal density for importance sampling.\n\nMCMC Sampling\nFirst, we will illustrate how to use the DIRT density as part of an MCMC sampler. The simplest sampler, which we demonstrate here, is an independence sampler using the DIRT density as a proposal density.\n\ndef potential(xs: torch.Tensor) -&gt; torch.Tensor:\n    return negloglik(xs) + neglogpri(xs)\n\nrs = dirt.reference.random(d=dirt.dim, n=1000)\nsamples, potentials_irt = dirt.eval_irt(rs)\npotentials_true = potential(samples)\n\nres = dt.run_independence_sampler(samples, potentials_irt, potentials_true)\nprint(f\"Acceptance rate: {res.acceptance_rate:.4f}\")\n\nAcceptance rate: 0.9100\n\n\nFigure 3 shows the first 20 points of the simulated Markov chain.\n\n\nCode\nfig, ax = plt.subplots(figsize=(7, 3.5), sharex=True, sharey=True)\n\nax.pcolormesh(b_grid, g_grid, pdf_true)\nax.scatter(*res.xs[:20].T, c=\"white\", s=4)\nax.plot(*res.xs[:20].T, c=\"white\", lw=0.4)\n\nax.set_xlabel(r\"$\\beta$\")\nax.set_ylabel(r\"$\\gamma$\")\nax.set_box_aspect(1)\n\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3: The first 20 points of the simulated Markov chain, plotted on top of the true posterior density.\n\n\n\n\n\n\n\nImportance Sampling\nAs an alternative to MCMC, we can also apply importance sampling to reweight samples from the DIRT approximation appropriately.\n\nres = dt.run_importance_sampling(potentials_irt, potentials_true)\nprint(f\"ESS: {res.ess:.2f}\")\n\nESS: 976.20\n\n\nAs expected, the effective sample size (ESS) is almost equal to the number of samples.",
    "crumbs": [
      "Examples",
      "SIR Model"
    ]
  },
  {
    "objectID": "examples/index.html",
    "href": "examples/index.html",
    "title": "Examples",
    "section": "",
    "text": "This page contains examples demonstrating the use of \\(\\texttt{deep\\_tensor}\\).",
    "crumbs": [
      "Examples"
    ]
  }
]